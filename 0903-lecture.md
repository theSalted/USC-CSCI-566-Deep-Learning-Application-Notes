# Machine Learning and Deep Learning Applications

- AlphaGo (a breakout deep learning application)
- Autonmous Driving (getting pulled out due to accidents)
- ChatGPT

Many algorithm we use today were created before 2000

1958-1999 (Age of CS)
- Peceptron (1958)
- Backprop (1986)
- SVM (1992)
- ConvNet (1998)
- Graident Boosting Machine (GBM) (1999) (Google, Facetime, Amazon used to uses this in their recommender system a lot.)

2000-2010: Arrival of Big Data (Age of Social Media)
- Wikipedia (2001)
- flickr (2004)
- MTurk (2005)
- Netflix (2009) (The Netflix Challenge)
- Kaggle/ImageNet (2010)

2006-Now: Compute and Scaling (Age of TSMC???? or Foundary Model)
- Public cloud
- Nvidia CUDA
- Micro controller?
- TensorCore
- Server Clusters?

2010-Now: ML Systems
- Different frameworks

Question for self: Why pillars of DL happened at the time it happened

1. Success in deep neural networks with advancement in hardware

Opportunity with GPUs:
1. Successs in deep neural networks with advancement in hardware 
2. Story of AlphaFold (amounts of data available really matters!!!)

1. Success in deep neural networks with advancement in hardware
    - Fast computation with tensor operations/algebra
2. Maturity in learning systems (mxnet, DL4J, ONNX, PyTorch, TF, PaddlePaddle) (These frameworks are called learning systems)

Case Study: Ingredient of AlexNet 

Methods:
- SGD
- Dropout
- ConvNet
- Initialization

Data:
- ImageNet

...

AlexNet now available in 100 lines of python in a few hours

Model- Data - Compute
^
ML Systems

- Gradient Descent was developed for graphic model

- Ubiquitous Applications
    - From speech recongnition to predictive analytics
- Expensive Data Sets
    - Datasets are more available
- Enhanced Algorithms and Computational Power
    - Depp Learning breakthhroughs
        - Deep learning breathrought: Transformers, GANS, and reinforcement learning.
        - High-performance computing: GPUS and TPUs facilitating faster model training
    - Advanced Development paradigms
        - The rise of AutoML

In ML we have bottleneck everywhere

# Review of Mathmatical Foundation - Part 1

## Linear Algebra

- Vector space: A real vector space V= (V, +, \dot) is a set V with two operations
https://xkcd.com/1838/

Dimensions
The number of basis vectors of a vector space V is the dimension of V and denoted by dim(V)

Rank
The number of linearly independent columns of a matrix A = Row^m*n. This quals the number of linearly independent rows of A.

L1 Norm looks like a Square
L2 Norm looks like a Circle


