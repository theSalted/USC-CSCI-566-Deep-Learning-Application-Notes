# CSCI 566 Deep Learning Applications

Professor Yan Liu

Covers Machine Learning, CV, NLP, Robotics, and othe AI-related fields in the last decade.

**New cool stuff**: guest lectures from indutry academia, and more to discuss their life in ML and Data Science.

Also teached Mathmetics required for deep learning.

Prequsites:
- Probability and Statstical Learning
    - Density function, loss function, cross-validation
- Supervised Learning
    - Nearest Neigbor, Kernels, Random Forest
- Unsupervised Learning
    - Clustering, PCA, SVD

Classes Afterwards:

- *Machcine learning theory*
- Advanced topics in deep learning 
- Deep reforcement learning
- Fairness, roboutness, and interpretability


Materials will be distributed at Piazza (passcode: csci566)
https://piazza.com/usc/fall2024/csci566

Instructor Office Hour:
Time Wed 10am

Next Week Mathmatical Foundation
1. Linear Algebra
2. Matrix decompostion
3. Vector Calculus
4. Probability and Distirbuition
5. Continuous optimization


## Books

Deep Leaning (MIT Prss) By Ian Goodfellow

Mathmetics for Machine Learning (Cambridge University Press)

Deisenroth, A. Ado Faisal

## Machine Learning and Deep Leaning

### TAs:

- James Enouen (enouen@usc.edu)
    - Interpretable Models for DL and ML (Feature interaction)
    - Interpretability, DL Theory, Casual Inference
- Wen Ye (yewen@usc.edu)
    - Building Time Series Foundation Model
        - Task agnostic model for modeling time series
    - Time Series Multi-Step Reasoning
        - Can model not be contrained to certain types well defined questions?

Time-series are any data that changed over time

- Sam Griesemer (grieseme@usc.edu)
    - Bringing in ML in more fields of natrual sciences
    - 
- Duygu Nur Yaidiz
    - Continual learning refers to a model's ability to learn from new data over time without fogetting previously aquired knowledge.
    - Continuously updated models remian effective and relevant as conditions changes. It reduces rth need for retraining from scratch.
    - The main challenge - Catastrophic Forgetting: Te tendecy of a neural netowrk completely overwrite ild information when new data is introduced, which is a major challenge in CL.
    - Common Approaches:
        - Elastic Weight Consolidation
        - Replay Mechanicsms
        - Dynamic Architecture

**Q-learning**
- Bingjie Tang
    
